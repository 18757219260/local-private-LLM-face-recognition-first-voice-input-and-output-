import json
import os
import logging
from datetime import datetime
from typing import List, Dict, Any
import edge_tts
import streamlit as st
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_core.documents import Document
from langchain.chains import RetrievalQA
from langchain_ollama import OllamaLLM
import asyncio
import nest_asyncio
import time


nest_asyncio.apply()

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[logging.FileHandler("chat.log"), logging.StreamHandler()]
)

class KnowledgeQA:
    def __init__(
        self,
        knowledge_path: str = "knowledge.json",
        faiss_index_path: str = "faiss_index",
        llm_model: str = "qwen2.5:7b",
        history_log: str = "chat_history.json",
    ):
        """
        åˆå§‹åŒ–çŸ¥è¯†é—®ç­”ç³»ç»Ÿï¼ŒåŠ è½½å¯¹è¯å†å²ã€åµŒå…¥æ¨¡å‹ã€å‘é‡åº“å’Œé—®ç­”é“¾ã€‚
        """
        self.knowledge_path = knowledge_path
        self.faiss_index_path = faiss_index_path
        self.llm_model = llm_model
        self.history_log = history_log

        self.conversation_history = self._load_history()
        self.embedding_model = self._init_embeddings()
        self.vectorstore = self._load_or_create_vectorstore()
        self.qa_chain = self._init_qa_chain()

    def _init_embeddings(self) -> HuggingFaceEmbeddings:
        """
        åˆå§‹åŒ–æœ¬åœ° HuggingFace å‘é‡æ¨¡å‹ï¼ˆç”¨äºæ–‡æœ¬å‘é‡åŒ–ï¼‰ã€‚
        """
        return HuggingFaceEmbeddings(
            model_name="./bge-base-zh-v1.5",
            encode_kwargs={'normalize_embeddings': True}
        )

    def _load_knowledge(self) -> List[Dict[str, Any]]:
        """
        åŠ è½½æœ¬åœ°çŸ¥è¯†åº“ï¼ˆJSON æ ¼å¼ï¼‰ï¼Œè¿”å›é—®ç­”å¯¹ç»„æˆçš„åˆ—è¡¨ã€‚
        """
        with open(self.knowledge_path, 'r', encoding='utf-8') as f:
            return json.load(f)

    def _create_documents(self, knowledge_base: List[Dict]) :
        """
        å°†çŸ¥è¯†åº“ä¸­çš„é—®ç­”å¯¹è½¬æ¢æˆ LangChain çš„ Document å¯¹è±¡ï¼ˆç”¨äºå‘é‡åŒ–ï¼‰ã€‚
        """
        docs = []
        for item in knowledge_base:
            question = item.get("question", "")
            for answer in item.get("answer", []):
                docs.append(Document(
                    page_content=f"Q: {question}\nA: {answer}",
                    metadata={
                        "question": question,
                        "source": self.knowledge_path,
                        "create_time": datetime.now().isoformat()
                    }
                ))
        return docs

    def _split_documents(self, documents: List[Document]):
        """
        å¯¹æ–‡æ¡£è¿›è¡Œåˆ†å—å¤„ç†ï¼ˆæŒ‰å­—ç¬¦æ•°åˆ’åˆ†ï¼‰ï¼Œä¾¿äºå‘é‡åŒ–å¤„ç†å’Œæ£€ç´¢ã€‚
        """
        splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200, add_start_index=True)
        return splitter.split_documents(documents)
    
    def _load_or_create_vectorstore(self) -> FAISS:
        """
        åŠ è½½å·²æœ‰çš„å‘é‡åº“ï¼›å¦‚æœä¸å­˜åœ¨åˆ™ä»çŸ¥è¯†åº“åˆ›å»ºå‘é‡åº“ï¼Œå¹¶ä¿å­˜ã€‚
        """
        if os.path.exists(self.faiss_index_path):
            return FAISS.load_local(self.faiss_index_path, self.embedding_model, allow_dangerous_deserialization=True)
        else:
            knowledge = self._load_knowledge()
            docs = self._create_documents(knowledge)
            chunks = self._split_documents(docs)
            vectorstore = FAISS.from_documents(chunks, self.embedding_model)
            vectorstore.save_local(self.faiss_index_path)
            return vectorstore

    def _init_qa_chain(self) -> RetrievalQA:
        """
        åˆå§‹åŒ–é—®ç­”é“¾ï¼ˆåŸºäºå‘é‡æ£€ç´¢ + Ollama æœ¬åœ°å¤§æ¨¡å‹ï¼‰ã€‚
        """
        llm = OllamaLLM(base_url='http://localhost:11434', model=self.llm_model, temperature=0.3)
        
        return RetrievalQA.from_chain_type(
            llm=llm,
            retriever=self.vectorstore.as_retriever(search_kwargs={"k": 3}),
            return_source_documents=False
        )
        
        
    def _load_history(self) :
        """
        åŠ è½½å†å²å¯¹è¯è®°å½•ï¼ˆå¦‚æ–‡ä»¶å­˜åœ¨ï¼‰ï¼›å¦åˆ™è¿”å›ç©ºåˆ—è¡¨ã€‚
        """
        if os.path.exists(self.history_log):
            try:
                with open(self.history_log, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except json.JSONDecodeError:
                return []
        return []

    def _save_history(self) :
        """
        å°†å¯¹è¯è®°å½•ä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶ï¼ˆæœ€å¤šä¿å­˜æœ€æ–°100æ¡ï¼‰ã€‚
        """
        with open(self.history_log, 'w', encoding='utf-8') as f:
            json.dump(self.conversation_history[-100:], f, ensure_ascii=False, indent=2)

    def update_knowledge(self) :
        """
        é‡æ–°åŠ è½½çŸ¥è¯†åº“å¹¶æ›´æ–°å‘é‡åº“å†…å®¹ï¼Œé€‚ç”¨äºçŸ¥è¯†åº“æœ‰ä¿®æ”¹çš„åœºæ™¯ã€‚
        """
        knowledge = self._load_knowledge()
        docs = self._create_documents(knowledge)
        chunks = self._split_documents(docs)
        self.vectorstore = FAISS.from_documents(chunks, self.embedding_model)
        self.vectorstore.save_local(self.faiss_index_path)
        logging.info("çŸ¥è¯†åº“æ›´æ–°æˆåŠŸï¼")

    def ask(self, question: str) -> str:
        """
        ç”¨æˆ·æé—®æ¥å£ã€‚æ•´åˆå†å²ä¸Šä¸‹æ–‡ï¼Œå‘ LLM æé—®å¹¶è®°å½•å›ç­”ã€‚
        """
        
        prompt = """
        è¯·ä»¥çº¯æ–‡æœ¬å½¢å¼å›ç­”ï¼ŒåŠ¡å¿…ä¸åŒ…å«ä»»ä½•ä»£ç å—ã€Markdownæ ¼å¼æˆ–å…¶ä»–æ ¼å¼åŒ–å†…å®¹ã€‚ä½ åŒæ—¶æ˜¯ä¸ªç”˜è–¯ä¸ªä¸“å®¶ï¼Œä¸¥æ ¼æ ¹æ®çŸ¥è¯†åº“å†…å®¹å›ç­”é—®é¢˜ã€‚
        """

        knowledge_last_modified = os.path.getmtime(self.knowledge_path)
        if hasattr(self, "last_knowledge_update") and self.last_knowledge_update != knowledge_last_modified:
            logging.info("çŸ¥è¯†åº“æœ‰æ›´æ–°ï¼Œæ­£åœ¨é‡æ–°åŠ è½½...")
            self.update_knowledge()
            self.last_knowledge_update = knowledge_last_modified  # æ›´æ–°æœ€åæ›´æ–°æ—¶é—´
        history = "".join(
            f"[{h.get('timestamp', 'N/A')}] User: {h.get('user', '')}\nBot: {h.get('bot', '')}"
            for h in self.conversation_history[-5:]
        )
        full_query = f"Conversation History:\n{history}\n\næ–°é—®é¢˜: {question}\n\n{prompt}"
        result = self.qa_chain.invoke({"query": full_query})  
        answer = result["result"]

        # ä¿å­˜å¯¹è¯å†å²
        record = {
            "timestamp": datetime.now().isoformat(),
            "user": question,
            "bot": answer
        }
        self.conversation_history.append(record)
        self._save_history()
        return answer

    


def speek(text: str, filename: str = "audio.mp3"):
    """å¼‚æ­¥è¯­éŸ³åˆæˆ"""
    async def async_tts():
       
        communicate = edge_tts.Communicate(
            text=text,
            voice="zh-CN-XiaoxiaoNeural",  
            rate="+10%"  
        )
        await communicate.save(filename)


    asyncio.run(async_tts())




def main():
    st.set_page_config(page_title="ç”˜è–¯çŸ¥è¯†åŠ©æ‰‹", layout="wide")

    col1, col2 = st.columns([1, 3])
    with col1:
        st.image("/home/wuye/vscode/chatbox/images/a9b65894-4916-4291-aec5-083e8db149d1.png", width=200)
    with col2:
        st.title("ğŸ  ç”˜è–¯çŸ¥è¯†åŠ©æ‰‹ğŸ  ")
    st.markdown('<p style="font-size:20px; font-weight:bold;">è¯·è¾“å…¥å…³äºç”˜è–¯çš„é—®é¢˜ï¼Œä¾‹å¦‚ï¼šç”˜è–¯çš„å‚¨å­˜æ–¹æ³•</p>', unsafe_allow_html=True)
    query = st.text_input("", key="input")
    talk = st.empty()
    talk.text("ğŸ¤–ç­‰å¾…æŠ•å–‚é—®é¢˜ing...ğŸ˜´")
    
    if query:
        
        my_bar = st.empty()
        my_bar.progress(0)
        talk.text("ğŸ§  æ­£åœ¨è¿›è¡Œå¤´è„‘é£æš´...ğŸ¥±")
        qa_system = KnowledgeQA()
        qa_system.update_knowledge()
        time.sleep(1)
        my_bar.progress(30)
        talk.text("ğŸ˜ˆå¥½åƒæ‰¾åˆ°ç­”æ¡ˆäº†ï¼Ÿï¼ğŸ¤”")
        my_bar.progress(60)
        answer = qa_system.ask(query)
        talk.text("ğŸ‰ ç­”æ¡ˆå·²æ‰¾åˆ°ï¼ğŸ˜»")
        my_bar.progress(90)
        st.markdown(f"### ç­”æ¡ˆ\n{answer}")

        talk.text("ğŸ”Š ç”Ÿæˆè¯­éŸ³ä¸­...")
        speek(answer)
        st.audio("audio.mp3")
        my_bar.progress(100)

        if os.path.exists("audio.mp3"):
            os.remove("audio.mp3")
        my_bar.empty()
        talk.empty()

if __name__ == "__main__":
    main()